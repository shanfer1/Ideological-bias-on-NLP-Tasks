{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmajeed2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train=pd.read_csv('train_file_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     topic\n",
      "0    In an exclusive CNN interview, former Universi...    sports\n",
      "1    A NASA scientist and her friend, who summited ...    sports\n",
      "2    Tom Brady and the Tampa Bay Buccaneers beat th...    sports\n",
      "3    American MMA fighter Anthony \"Rumble\" Johnson ...    sports\n",
      "4    \"It felt electric from the time we took the fi...    sports\n",
      "..                                                 ...       ...\n",
      "254  The James Webb Space Telescope has snapped a r...     Space\n",
      "255  NASA’s Artemis I mega moon rocket could face d...     Space\n",
      "256  Need to get to Mars? This inflatable shield co...     Space\n",
      "257  An Iranian court has issued the first death se...  Religion\n",
      "258  \"Twitter has given a blue check verified mark ...  Religion\n",
      "\n",
      "[259 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train = db_train[['text', 'topic']].copy()\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test=pd.read_csv('test_file_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     topic\n",
      "0    'Harry Potter' star Ralph Fiennes reveals he w...    sports\n",
      "1    UVA shooting: Football team cancels final home...    sports\n",
      "2    Aaron Rodgers admits frustration over coach Ma...    sports\n",
      "3    New Jersey Fish and Wildlife to vote on reinst...    sports\n",
      "4    Here's how a recount in Arizona's gubernatoria...    sports\n",
      "..                                                 ...       ...\n",
      "258  Charlamagne Tha God: Dems 'got nobody' to defe...  Religion\n",
      "259  New memoir details Christian D-Day hero's 'fai...  Religion\n",
      "260  OPINION: @TuckerCarlson: Democracy is a faith-...  Religion\n",
      "261  Patricia Heaton tells Christians 'disappointed...  Religion\n",
      "262  KEEP THE FAITH: “Everybody Loves Raymond” actr...  Religion\n",
      "\n",
      "[263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test = db_test[['text', 'topic']].copy()\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def text_preprocess(value):\n",
    "    value['text'] = value['text'].str.lower() #lowercase all words\n",
    "    value['text']=value['text'].str.split().map(lambda sl: \" \".join(s for s in sl if len(s) > 2)) #remove word less than 2 char\n",
    "    value['text'] = value['text'].str.replace(r'\\d+','') #removing numbers\n",
    "    value['text'] = value['text'].str.replace(r'[ ](?=[ ])|[^-_,A-Za-z0-9 ]+', \"\") #removing special characters \n",
    "    value['tokenized_sents'] = value.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "    value['tokenized_sentences']= value['tokenized_sentences'].apply(lambda x: [item for item in x if item not in stop_list]) #removing stopwords\n",
    "    value['tokenized_sentences'] = value['tokenized_sentences'].apply(lambda x : [stemmer.stem(y) for y in x]) #stemming \n",
    "    value['tokenized_sentences'] = value['tokenized_sentences'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) #lemmatizing \n",
    "    value['clean_text'] = value['tokenized_sentences'].str.join(' ') #converting back to normal string from tokenized string \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmajeed2\\AppData\\Local\\Temp\\ipykernel_12216\\3336950754.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  value['text'] = value['text'].str.replace(r'\\d+','') #removing numbers\n",
      "C:\\Users\\mmajeed2\\AppData\\Local\\Temp\\ipykernel_12216\\3336950754.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  value['text'] = value['text'].str.replace(r'[ ](?=[ ])|[^-_,A-Za-z0-9 ]+', \"\") #removing special characters\n"
     ]
    }
   ],
   "source": [
    "df_train = text_preprocess(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mmajeed2/nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mmajeed2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmajeed2\\Desktop\\workspace\\SMM\\Idelogical-Bias\\randomforest.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mmajeed2/Desktop/workspace/SMM/Idelogical-Bias/randomforest.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_train[\u001b[39m'\u001b[39m\u001b[39mtokenized_sentences\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: nltk\u001b[39m.\u001b[39;49mword_tokenize(row[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:8848\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8837\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8839\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   8840\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   8841\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8846\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   8847\u001b[0m )\n\u001b[1;32m-> 8848\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:733\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    731\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:857\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 857\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    859\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    872\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    874\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    875\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    876\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    877\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\mmajeed2\\Desktop\\workspace\\SMM\\Idelogical-Bias\\randomforest.ipynb Cell 8\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mmajeed2/Desktop/workspace/SMM/Idelogical-Bias/randomforest.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_train[\u001b[39m'\u001b[39m\u001b[39mtokenized_sentences\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: nltk\u001b[39m.\u001b[39;49mword_tokenize(row[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\mmajeed2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mmajeed2/nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\mmajeed2\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mmajeed2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df_train['tokenized_sentences'] = df_train.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "# df_train['tokenized_sentences']= df_train['tokenized_sentences'].apply(lambda x: [item for item in x if item not in stop_list]) #removing stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test ,y_train,y_test = df_train['text'], df_test['text'], df_train['topic'], df_test['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      In an exclusive CNN interview, former Universi...\n",
       "1      A NASA scientist and her friend, who summited ...\n",
       "2      Tom Brady and the Tampa Bay Buccaneers beat th...\n",
       "3      American MMA fighter Anthony \"Rumble\" Johnson ...\n",
       "4      \"It felt electric from the time we took the fi...\n",
       "                             ...                        \n",
       "254    The James Webb Space Telescope has snapped a r...\n",
       "255    NASA’s Artemis I mega moon rocket could face d...\n",
       "256    Need to get to Mars? This inflatable shield co...\n",
       "257    An Iranian court has issued the first death se...\n",
       "258    \"Twitter has given a blue check verified mark ...\n",
       "Name: text, Length: 259, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(value):\n",
    "    value['text'] = value['text'].str.lower() #lowercase all words\n",
    "    value['text']=value['text'].str.split().map(lambda sl: \" \".join(s for s in sl if len(s) > 2)) #remove word less than 2 char\n",
    "    value['text'] = value['text'].str.replace(r'\\d+','') #removing numbers\n",
    "    value['text'] = value['text'].str.replace(r'[ ](?=[ ])|[^-_,A-Za-z0-9 ]+', \"\") #removing special characters \n",
    "    value['tokenized_sentences'] = value.apply(lambda row: nltk.word_tokenize(row['text']), axis=1) #tokenizing \n",
    "    value['tokenized_sentences']= value['tokenized_sentences'].apply(lambda x: [item for item in x if item not in stop_list]) #removing stopwords\n",
    "    value['tokenized_sentences'] = value['tokenized_sentences'].apply(lambda x : [stemmer.stem(y) for y in x]) #stemming \n",
    "    value['tokenized_sentences'] = value['tokenized_sentences'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) #lemmatizing \n",
    "    value['clean_string'] = value['tokenized_sentences'].str.join(' ') #converting back to normal string from tokenized string \n",
    "    return value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a430ce3e785eaed4e3af6085b2c31bfb94cdf8f087fec0080a3cf792164695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
